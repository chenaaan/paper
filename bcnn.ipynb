{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模块\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "import torch\n",
    "import torchvision\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from fine_tuning_config_file import *\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#采用gpu训练模型\n",
    "use_gpu = GPU_MODE\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据转换，为了扩增数据\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "#加载数据集\n",
    "data_dir = 'WCE'\n",
    "dsets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "         for x in ['train', 'val']}\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=25)\n",
    "                for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练网络模型\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                mode='train'\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                mode='val'\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            TP=0\n",
    "            TN=0\n",
    "            FN=0\n",
    "            FP=0\n",
    "            TP_item=0\n",
    "            TN_item=0\n",
    "            FN_item=0\n",
    "            FP_item=0\n",
    "            counter=0\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                inputs, labels = data\n",
    "                #print(inputs.size())\n",
    "                # wrap them in Variable\n",
    "                inputs = torch.autograd.Variable(inputs.float().cuda())                            \n",
    "                labels = torch.autograd.Variable(labels.long().cuda())\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                #------------保存文件夹--------------\n",
    "                #preds=preds.cpu().numpy()\n",
    "                #0是abnormal，1是normal\n",
    "               # print(preds)\n",
    "                if counter==0:\n",
    "                    phase_pred=preds.cpu().numpy()\n",
    "                    phase_label=labels.cpu().numpy()\n",
    "                else:\n",
    "                    phase_pred = np.append(phase_pred, preds.cpu().numpy())\n",
    "                    phase_label = np.append(phase_label,labels.cpu().numpy())\n",
    "                # TP    predict 和 label 同时为1\n",
    "                TP += ((preds == 1) & (labels.data == 1)).cpu().sum()\n",
    "                    #TP_item +=torch.sum((preds[i] == 1) & (labels.data[i] == 1))\n",
    "                    #print(TP_item)\n",
    "                    # TN    predict 和 label 同时为0\n",
    "                TN += ((preds == 0) & (labels.data == 0)).cpu().sum()\n",
    "                    #print(TN_item)\n",
    "                    # FN    predict 0 label 1\n",
    "                FN += ((preds == 0) & (labels.data == 1)).cpu().sum()\n",
    "                    # FP    predict 1 label 0\n",
    "                FP += ((preds == 1) & (labels.data == 0)).cpu().sum()\n",
    "                loss = criterion(outputs, labels)\n",
    "                # print('loss done')                \n",
    "                # Just so that you can keep track that something's happening and don't feel like the program isn't running.\n",
    "                # if counter%10==0:\n",
    "                #     print(\"Reached iteration \",counter)\n",
    "                counter+=1\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    # print('loss backward')\n",
    "                    loss.backward()\n",
    "                    # print('done loss backward')\n",
    "                    optimizer.step()\n",
    "                    # print('done optim')\n",
    "                # print evaluation statistics\n",
    "                try:\n",
    "                    # running_loss += loss.data[0]\n",
    "                    running_loss += loss.item()\n",
    "                    # print(labels.data)\n",
    "                    # print(preds)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    \"\"\"TP+=TP_item\n",
    "                    TN+=TN_item\n",
    "                    FN+=FN_item\n",
    "                    FP+=FP_item\"\"\"\n",
    "                    # print('running correct =',running_corrects)\n",
    "                except:\n",
    "                    print('unexpected error, could not calculate loss or do a sum.')\n",
    "            print('trying epoch loss')\n",
    "            print(\"TP:\",TP)\n",
    "            print(\"TN:\",TN)\n",
    "            print(\"FN:\",FN)\n",
    "            print(\"FP:\",FP)\n",
    "            r = TP.float() / (TP + FN).float()\n",
    "            TNR= TN .float()/ (FP + TN).float()\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects.item() / float(dset_sizes[phase])  \n",
    "            epoch_auc = roc_auc_score(phase_label, phase_pred)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Auc: {:.4f} recall: {:.4f} specificity: {:.4f} '.format(\n",
    "                phase, epoch_loss, epoch_acc,epoch_auc,r,TNR))\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                \"\"\"if USE_TENSORBOARD:\n",
    "                    foo.add_scalar_value('epoch_loss',epoch_loss,step=epoch)\n",
    "                    foo.add_scalar_value('epoch_acc',epoch_acc,step=epoch)\"\"\"\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    print('new best accuracy = ',best_acc)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用迁移学习搭建自己的网络\n",
    "class BCNN(torch.nn.Module):\n",
    "    \"\"\"B-CNN for CUB200.\n",
    "\n",
    "    The B-CNN model is illustrated as follows.\n",
    "    conv1^2 (64) -> pool1 -> conv2^2 (128) -> pool2 -> conv3^3 (256) -> pool3\n",
    "    -> conv4^3 (512) -> pool4 -> conv5^3 (512) -> bilinear pooling\n",
    "    -> sqrt-normalize -> L2-normalize -> fc (200).\n",
    "    The network accepts a 3*448*448 input, and the pool5 activation has shape\n",
    "    512*28*28 since we down-sample 5 times.\n",
    "    vgg16：conv1^2 (64) -> pool1 -> conv2^2 (128) -> pool2 -> conv3^3 (256) -> pool3\n",
    "    -> conv4^3 (512) -> pool4 -> conv5^3 (512) \n",
    "    Attributes:\n",
    "        features, torch.nn.Module: Convolution and pooling layers.\n",
    "        fc, torch.nn.Module: 200.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        torch.nn.Module.__init__(self)\n",
    "        # Convolution and pooling layers of VGG-16.\n",
    "        self.features = torchvision.models.vgg16(pretrained=False,progress=True).features#获取vgg16的特征\n",
    "        self.features = torch.nn.Sequential(*list(self.features.children())#获取零到最后第二层的特征\n",
    "                                            [:-1])# Remove pool5.\n",
    "        #self.features2 = torch.nn.Sequential(*list(self.features.children())#获取零到最后第二层的特征\n",
    "          #                                  [:-])\n",
    "        \n",
    "        # Linear classifier.\n",
    "        self.fc = torch.nn.Linear(512**2, 2)#定义一个输入特征为512*2，输出特征为200的线性层\n",
    "        #self.bn = torch.nn.BatchNorm1d(512**2)#加入\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            X, torch.autograd.Variable of shape N*3*448*448.\n",
    "\n",
    "        Returns:\n",
    "            Score, torch.autograd.Variable of shape N*200.\n",
    "        \"\"\"\n",
    "        N = X.size()[0]#n为x的第一维形状\n",
    "        assert X.size() == (N, 3, 224, 224)#判断x的形状是否为（n，3，448，448）\n",
    "        X = self.features(X)#？？\n",
    "        assert X.size() == (N, 512, 14, 14)#判断x的形状是否为（n，512，28，28）\n",
    "        X = X.view(N, 512, 14**2)#转换x的形状为(n，512，28*28)\n",
    "        X = torch.bmm(X, torch.transpose(X, 1, 2)) / (14**2)  # Bilinear  x矩阵乘x的转置然后除28*28\n",
    "        assert X.size() == (N, 512, 512)#判断x的形状是否为\n",
    "        X = X.view(N, 512**2)\n",
    "        X = torch.sqrt(X + 1e-5)#开根号\n",
    "        X = torch.nn.functional.normalize(X)#进行参数规范化\n",
    "        #X = self.bn(X)\n",
    "        X = self.fc(X)#加入线性层，输出为（n，200）\n",
    "        assert X.size() == (N, 2)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改变训练模型的学习率\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.001\n",
      "trying epoch loss\n",
      "TP: tensor(9780)\n",
      "TN: tensor(6386)\n",
      "FN: tensor(1220)\n",
      "FP: tensor(2054)\n",
      "train Loss: 0.0389 Acc: 0.8316 Auc: 0.8229 recall: 0.8891 specificity: 0.7566 \n",
      "trying epoch loss\n",
      "TP: tensor(4502)\n",
      "TN: tensor(3030)\n",
      "FN: tensor(357)\n",
      "FP: tensor(810)\n",
      "val Loss: 0.0321 Acc: 0.8658 Auc: 0.8578 recall: 0.9265 specificity: 0.7891 \n",
      "new best accuracy =  0.8658466490401195\n",
      "Epoch 1/29\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ca6a0de2105e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Run the functions and save the best model in the function model_ft.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[1;32m---> 15\u001b[1;33m                        num_epochs=30)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-13d1804bc9a2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, lr_scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[1;31m#print(inputs.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;31m# wrap them in Variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft =BCNN().cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = FocalLoss(gamma=0)\n",
    "\n",
    "if use_gpu:\n",
    "    criterion=criterion.cuda()\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "optimizer_ft = torch.optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)\n",
    "\n",
    "# Save model\n",
    "model_ft.save_state_dict('fine_tuned_best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.001\n",
      "trying epoch loss\n",
      "TP: tensor(9796)\n",
      "TN: tensor(6373)\n",
      "FN: tensor(1204)\n",
      "FP: tensor(2067)\n",
      "train Loss: 0.0386 Acc: 0.8317 Auc: 0.8228 recall: 0.8905 specificity: 0.7551 \n",
      "trying epoch loss\n",
      "TP: tensor(4477)\n",
      "TN: tensor(3088)\n",
      "FN: tensor(382)\n",
      "FP: tensor(752)\n",
      "val Loss: 0.0317 Acc: 0.8696 Auc: 0.8628 recall: 0.9214 specificity: 0.8042 \n",
      "new best accuracy =  0.869640188527417\n",
      "Epoch 1/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9924)\n",
      "TN: tensor(6690)\n",
      "FN: tensor(1076)\n",
      "FP: tensor(1750)\n",
      "train Loss: 0.0347 Acc: 0.8546 Auc: 0.8474 recall: 0.9022 specificity: 0.7927 \n",
      "trying epoch loss\n",
      "TP: tensor(4698)\n",
      "TN: tensor(2735)\n",
      "FN: tensor(161)\n",
      "FP: tensor(1105)\n",
      "val Loss: 0.0335 Acc: 0.8545 Auc: 0.8396 recall: 0.9669 specificity: 0.7122 \n",
      "Epoch 2/29\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b0f337fd57dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Run the functions and save the best model in the function model_ft.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[1;32m---> 15\u001b[1;33m                        num_epochs=30)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-13d1804bc9a2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, lr_scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[0mphase_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                     \u001b[0mphase_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                     \u001b[0mphase_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;31m# TP    predict 和 label 同时为1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft =BCNN().cuda()\n",
    "model_ft.load_state_dict(torch.load('models/model_ft_vgg16.pth'),strict=False)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = FocalLoss(gamma=0)\n",
    "\n",
    "if use_gpu:\n",
    "    criterion=criterion.cuda()\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "optimizer_ft = torch.optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "           Linear-31                    [-1, 2]         524,290\n",
      "================================================================\n",
      "Total params: 15,238,978\n",
      "Trainable params: 524,290\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.20\n",
      "Params size (MB): 58.13\n",
      "Estimated Total Size (MB): 276.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torchsummary import summary\n",
    "summary(BCNN().cuda(), input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                    [-1, 2]          50,178\n",
      "================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 14,764,866\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.59\n",
      "Params size (MB): 56.32\n",
      "Estimated Total Size (MB): 275.48\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_ft_vgg =torchvision.models.vgg16(pretrained=True).to(device)\n",
    "num_ftrs = model_ft_vgg.classifier[0].in_features\n",
    "feature_model = list(model_ft_vgg.classifier[:-6].children())\n",
    "feature_model.pop()            \n",
    "feature_model.append(torch.nn.Linear(num_ftrs, NUM_CLASSES))\n",
    "model_ft_vgg.classifier = torch.nn.Sequential(*feature_model).to(device)\n",
    "from torchsummary import summary\n",
    "summary(model_ft_vgg, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.001\n",
      "trying epoch loss\n",
      "TP: tensor(8828)\n",
      "TN: tensor(6190)\n",
      "FN: tensor(2172)\n",
      "FP: tensor(2250)\n",
      "train Loss: 0.1265 Acc: 0.7725 Auc: 0.7680 recall: 0.8025 specificity: 0.7334 \n",
      "trying epoch loss\n",
      "TP: tensor(4272)\n",
      "TN: tensor(2958)\n",
      "FN: tensor(587)\n",
      "FP: tensor(882)\n",
      "val Loss: 0.0980 Acc: 0.8311 Auc: 0.8248 recall: 0.8792 specificity: 0.7703 \n",
      "new best accuracy =  0.8311300149442464\n",
      "Epoch 1/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9018)\n",
      "TN: tensor(6414)\n",
      "FN: tensor(1982)\n",
      "FP: tensor(2026)\n",
      "train Loss: 0.1208 Acc: 0.7938 Auc: 0.7899 recall: 0.8198 specificity: 0.7600 \n",
      "trying epoch loss\n",
      "TP: tensor(4546)\n",
      "TN: tensor(2716)\n",
      "FN: tensor(313)\n",
      "FP: tensor(1124)\n",
      "val Loss: 0.1201 Acc: 0.8348 Auc: 0.8214 recall: 0.9356 specificity: 0.7073 \n",
      "new best accuracy =  0.8348085986895045\n",
      "Epoch 2/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9088)\n",
      "TN: tensor(6455)\n",
      "FN: tensor(1912)\n",
      "FP: tensor(1985)\n",
      "train Loss: 0.1219 Acc: 0.7995 Auc: 0.7955 recall: 0.8262 specificity: 0.7648 \n",
      "trying epoch loss\n",
      "TP: tensor(4179)\n",
      "TN: tensor(3050)\n",
      "FN: tensor(680)\n",
      "FP: tensor(790)\n",
      "val Loss: 0.1288 Acc: 0.8310 Auc: 0.8272 recall: 0.8601 specificity: 0.7943 \n",
      "Epoch 3/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9153)\n",
      "TN: tensor(6559)\n",
      "FN: tensor(1847)\n",
      "FP: tensor(1881)\n",
      "train Loss: 0.1235 Acc: 0.8082 Auc: 0.8046 recall: 0.8321 specificity: 0.7771 \n",
      "trying epoch loss\n",
      "TP: tensor(3297)\n",
      "TN: tensor(3572)\n",
      "FN: tensor(1562)\n",
      "FP: tensor(268)\n",
      "val Loss: 0.1534 Acc: 0.7896 Auc: 0.8044 recall: 0.6785 specificity: 0.9302 \n",
      "Epoch 4/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9136)\n",
      "TN: tensor(6511)\n",
      "FN: tensor(1864)\n",
      "FP: tensor(1929)\n",
      "train Loss: 0.1270 Acc: 0.8049 Auc: 0.8010 recall: 0.8305 specificity: 0.7714 \n",
      "trying epoch loss\n",
      "TP: tensor(4496)\n",
      "TN: tensor(2780)\n",
      "FN: tensor(363)\n",
      "FP: tensor(1060)\n",
      "val Loss: 0.1241 Acc: 0.8364 Auc: 0.8246 recall: 0.9253 specificity: 0.7240 \n",
      "new best accuracy =  0.8364179790780549\n",
      "Epoch 5/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9137)\n",
      "TN: tensor(6551)\n",
      "FN: tensor(1863)\n",
      "FP: tensor(1889)\n",
      "train Loss: 0.1300 Acc: 0.8070 Auc: 0.8034 recall: 0.8306 specificity: 0.7762 \n",
      "trying epoch loss\n",
      "TP: tensor(4210)\n",
      "TN: tensor(3197)\n",
      "FN: tensor(649)\n",
      "FP: tensor(643)\n",
      "val Loss: 0.1122 Acc: 0.8515 Auc: 0.8495 recall: 0.8664 specificity: 0.8326 \n",
      "new best accuracy =  0.8514771812852052\n",
      "Epoch 6/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9174)\n",
      "TN: tensor(6572)\n",
      "FN: tensor(1826)\n",
      "FP: tensor(1868)\n",
      "train Loss: 0.1310 Acc: 0.8100 Auc: 0.8063 recall: 0.8340 specificity: 0.7787 \n",
      "trying epoch loss\n",
      "TP: tensor(3124)\n",
      "TN: tensor(3610)\n",
      "FN: tensor(1735)\n",
      "FP: tensor(230)\n",
      "val Loss: 0.1754 Acc: 0.7741 Auc: 0.7915 recall: 0.6429 specificity: 0.9401 \n",
      "Epoch 7/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9230)\n",
      "TN: tensor(6629)\n",
      "FN: tensor(1770)\n",
      "FP: tensor(1811)\n",
      "train Loss: 0.1272 Acc: 0.8158 Auc: 0.8123 recall: 0.8391 specificity: 0.7854 \n",
      "trying epoch loss\n",
      "TP: tensor(2530)\n",
      "TN: tensor(3724)\n",
      "FN: tensor(2329)\n",
      "FP: tensor(116)\n",
      "val Loss: 0.2695 Acc: 0.7189 Auc: 0.7452 recall: 0.5207 specificity: 0.9698 \n",
      "Epoch 8/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9225)\n",
      "TN: tensor(6609)\n",
      "FN: tensor(1775)\n",
      "FP: tensor(1831)\n",
      "train Loss: 0.1322 Acc: 0.8145 Auc: 0.8108 recall: 0.8386 specificity: 0.7831 \n",
      "trying epoch loss\n",
      "TP: tensor(4669)\n",
      "TN: tensor(2421)\n",
      "FN: tensor(190)\n",
      "FP: tensor(1419)\n",
      "val Loss: 0.1796 Acc: 0.8150 Auc: 0.7957 recall: 0.9609 specificity: 0.6305 \n",
      "Epoch 9/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9238)\n",
      "TN: tensor(6618)\n",
      "FN: tensor(1762)\n",
      "FP: tensor(1822)\n",
      "train Loss: 0.1313 Acc: 0.8156 Auc: 0.8120 recall: 0.8398 specificity: 0.7841 \n",
      "trying epoch loss\n",
      "TP: tensor(4628)\n",
      "TN: tensor(2466)\n",
      "FN: tensor(231)\n",
      "FP: tensor(1374)\n",
      "val Loss: 0.1856 Acc: 0.8155 Auc: 0.7973 recall: 0.9525 specificity: 0.6422 \n",
      "Epoch 10/29\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "trying epoch loss\n",
      "TP: tensor(9425)\n",
      "TN: tensor(6801)\n",
      "FN: tensor(1575)\n",
      "FP: tensor(1639)\n",
      "train Loss: 0.1015 Acc: 0.8347 Auc: 0.8313 recall: 0.8568 specificity: 0.8058 \n",
      "trying epoch loss\n",
      "TP: tensor(4043)\n",
      "TN: tensor(3278)\n",
      "FN: tensor(816)\n",
      "FP: tensor(562)\n",
      "val Loss: 0.1067 Acc: 0.8416 Auc: 0.8429 recall: 0.8321 specificity: 0.8536 \n",
      "Epoch 11/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9447)\n",
      "TN: tensor(6831)\n",
      "FN: tensor(1553)\n",
      "FP: tensor(1609)\n",
      "train Loss: 0.0914 Acc: 0.8373 Auc: 0.8341 recall: 0.8588 specificity: 0.8094 \n",
      "trying epoch loss\n",
      "TP: tensor(4066)\n",
      "TN: tensor(3254)\n",
      "FN: tensor(793)\n",
      "FP: tensor(586)\n",
      "val Loss: 0.0993 Acc: 0.8415 Auc: 0.8421 recall: 0.8368 specificity: 0.8474 \n",
      "Epoch 12/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9453)\n",
      "TN: tensor(6836)\n",
      "FN: tensor(1547)\n",
      "FP: tensor(1604)\n",
      "train Loss: 0.0830 Acc: 0.8379 Auc: 0.8347 recall: 0.8594 specificity: 0.8100 \n",
      "trying epoch loss\n",
      "TP: tensor(4310)\n",
      "TN: tensor(3045)\n",
      "FN: tensor(549)\n",
      "FP: tensor(795)\n",
      "val Loss: 0.0950 Acc: 0.8455 Auc: 0.8400 recall: 0.8870 specificity: 0.7930 \n",
      "Epoch 13/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9456)\n",
      "TN: tensor(6835)\n",
      "FN: tensor(1544)\n",
      "FP: tensor(1605)\n",
      "train Loss: 0.0830 Acc: 0.8380 Auc: 0.8347 recall: 0.8596 specificity: 0.8098 \n",
      "trying epoch loss\n",
      "TP: tensor(4122)\n",
      "TN: tensor(3209)\n",
      "FN: tensor(737)\n",
      "FP: tensor(631)\n",
      "val Loss: 0.0888 Acc: 0.8427 Auc: 0.8420 recall: 0.8483 specificity: 0.8357 \n",
      "Epoch 14/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9487)\n",
      "TN: tensor(6838)\n",
      "FN: tensor(1513)\n",
      "FP: tensor(1602)\n",
      "train Loss: 0.0795 Acc: 0.8398 Auc: 0.8363 recall: 0.8625 specificity: 0.8102 \n",
      "trying epoch loss\n",
      "TP: tensor(4507)\n",
      "TN: tensor(2843)\n",
      "FN: tensor(352)\n",
      "FP: tensor(997)\n",
      "val Loss: 0.0967 Acc: 0.8449 Auc: 0.8340 recall: 0.9276 specificity: 0.7404 \n",
      "Epoch 15/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9493)\n",
      "TN: tensor(6832)\n",
      "FN: tensor(1507)\n",
      "FP: tensor(1608)\n",
      "train Loss: 0.0780 Acc: 0.8398 Auc: 0.8362 recall: 0.8630 specificity: 0.8095 \n",
      "trying epoch loss\n",
      "TP: tensor(3841)\n",
      "TN: tensor(3377)\n",
      "FN: tensor(1018)\n",
      "FP: tensor(463)\n",
      "val Loss: 0.0928 Acc: 0.8298 Auc: 0.8350 recall: 0.7905 specificity: 0.8794 \n",
      "Epoch 16/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9471)\n",
      "TN: tensor(6840)\n",
      "FN: tensor(1529)\n",
      "FP: tensor(1600)\n",
      "train Loss: 0.0766 Acc: 0.8390 Auc: 0.8357 recall: 0.8610 specificity: 0.8104 \n",
      "trying epoch loss\n",
      "TP: tensor(4162)\n",
      "TN: tensor(3179)\n",
      "FN: tensor(697)\n",
      "FP: tensor(661)\n",
      "val Loss: 0.0804 Acc: 0.8439 Auc: 0.8422 recall: 0.8566 specificity: 0.8279 \n",
      "Epoch 17/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9544)\n",
      "TN: tensor(6884)\n",
      "FN: tensor(1456)\n",
      "FP: tensor(1556)\n",
      "train Loss: 0.0714 Acc: 0.8451 Auc: 0.8416 recall: 0.8676 specificity: 0.8156 \n",
      "trying epoch loss\n",
      "TP: tensor(4464)\n",
      "TN: tensor(2880)\n",
      "FN: tensor(395)\n",
      "FP: tensor(960)\n",
      "val Loss: 0.0871 Acc: 0.8442 Auc: 0.8344 recall: 0.9187 specificity: 0.7500 \n",
      "Epoch 18/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9523)\n",
      "TN: tensor(6873)\n",
      "FN: tensor(1477)\n",
      "FP: tensor(1567)\n",
      "train Loss: 0.0697 Acc: 0.8434 Auc: 0.8400 recall: 0.8657 specificity: 0.8143 \n",
      "trying epoch loss\n",
      "TP: tensor(4208)\n",
      "TN: tensor(3196)\n",
      "FN: tensor(651)\n",
      "FP: tensor(644)\n",
      "val Loss: 0.0743 Acc: 0.8511 Auc: 0.8492 recall: 0.8660 specificity: 0.8323 \n",
      "Epoch 19/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9544)\n",
      "TN: tensor(6865)\n",
      "FN: tensor(1456)\n",
      "FP: tensor(1575)\n",
      "train Loss: 0.0680 Acc: 0.8441 Auc: 0.8405 recall: 0.8676 specificity: 0.8134 \n",
      "trying epoch loss\n",
      "TP: tensor(4167)\n",
      "TN: tensor(3265)\n",
      "FN: tensor(692)\n",
      "FP: tensor(575)\n",
      "val Loss: 0.0742 Acc: 0.8544 Auc: 0.8539 recall: 0.8576 specificity: 0.8503 \n",
      "new best accuracy =  0.8543510748361881\n",
      "Epoch 20/29\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "trying epoch loss\n",
      "TP: tensor(9499)\n",
      "TN: tensor(6895)\n",
      "FN: tensor(1501)\n",
      "FP: tensor(1545)\n",
      "train Loss: 0.0671 Acc: 0.8433 Auc: 0.8402 recall: 0.8635 specificity: 0.8169 \n",
      "trying epoch loss\n",
      "TP: tensor(4211)\n",
      "TN: tensor(3213)\n",
      "FN: tensor(648)\n",
      "FP: tensor(627)\n",
      "val Loss: 0.0738 Acc: 0.8534 Auc: 0.8517 recall: 0.8666 specificity: 0.8367 \n",
      "Epoch 21/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9497)\n",
      "TN: tensor(6836)\n",
      "FN: tensor(1503)\n",
      "FP: tensor(1604)\n",
      "train Loss: 0.0671 Acc: 0.8402 Auc: 0.8367 recall: 0.8634 specificity: 0.8100 \n",
      "trying epoch loss\n",
      "TP: tensor(4349)\n",
      "TN: tensor(3057)\n",
      "FN: tensor(510)\n",
      "FP: tensor(783)\n",
      "val Loss: 0.0750 Acc: 0.8514 Auc: 0.8456 recall: 0.8950 specificity: 0.7961 \n",
      "Epoch 22/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9555)\n",
      "TN: tensor(6896)\n",
      "FN: tensor(1445)\n",
      "FP: tensor(1544)\n",
      "train Loss: 0.0671 Acc: 0.8462 Auc: 0.8428 recall: 0.8686 specificity: 0.8171 \n",
      "trying epoch loss\n",
      "TP: tensor(4282)\n",
      "TN: tensor(3129)\n",
      "FN: tensor(577)\n",
      "FP: tensor(711)\n",
      "val Loss: 0.0735 Acc: 0.8519 Auc: 0.8480 recall: 0.8813 specificity: 0.8148 \n",
      "Epoch 23/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9567)\n",
      "TN: tensor(6887)\n",
      "FN: tensor(1433)\n",
      "FP: tensor(1553)\n",
      "train Loss: 0.0634 Acc: 0.8464 Auc: 0.8429 recall: 0.8697 specificity: 0.8160 \n",
      "trying epoch loss\n",
      "TP: tensor(4338)\n",
      "TN: tensor(3086)\n",
      "FN: tensor(521)\n",
      "FP: tensor(754)\n",
      "val Loss: 0.0736 Acc: 0.8534 Auc: 0.8482 recall: 0.8928 specificity: 0.8036 \n",
      "Epoch 24/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9595)\n",
      "TN: tensor(6920)\n",
      "FN: tensor(1405)\n",
      "FP: tensor(1520)\n",
      "train Loss: 0.0644 Acc: 0.8495 Auc: 0.8461 recall: 0.8723 specificity: 0.8199 \n",
      "trying epoch loss\n",
      "TP: tensor(4252)\n",
      "TN: tensor(3158)\n",
      "FN: tensor(607)\n",
      "FP: tensor(682)\n",
      "val Loss: 0.0728 Acc: 0.8518 Auc: 0.8487 recall: 0.8751 specificity: 0.8224 \n",
      "Epoch 25/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9567)\n",
      "TN: tensor(6895)\n",
      "FN: tensor(1433)\n",
      "FP: tensor(1545)\n",
      "train Loss: 0.0650 Acc: 0.8468 Auc: 0.8433 recall: 0.8697 specificity: 0.8169 \n",
      "trying epoch loss\n",
      "TP: tensor(4292)\n",
      "TN: tensor(3114)\n",
      "FN: tensor(567)\n",
      "FP: tensor(726)\n",
      "val Loss: 0.0728 Acc: 0.8514 Auc: 0.8471 recall: 0.8833 specificity: 0.8109 \n",
      "Epoch 26/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9560)\n",
      "TN: tensor(6876)\n",
      "FN: tensor(1440)\n",
      "FP: tensor(1564)\n",
      "train Loss: 0.0653 Acc: 0.8455 Auc: 0.8419 recall: 0.8691 specificity: 0.8147 \n",
      "trying epoch loss\n",
      "TP: tensor(4339)\n",
      "TN: tensor(3063)\n",
      "FN: tensor(520)\n",
      "FP: tensor(777)\n",
      "val Loss: 0.0736 Acc: 0.8509 Auc: 0.8453 recall: 0.8930 specificity: 0.7977 \n",
      "Epoch 27/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9549)\n",
      "TN: tensor(6893)\n",
      "FN: tensor(1451)\n",
      "FP: tensor(1547)\n",
      "train Loss: 0.0642 Acc: 0.8458 Auc: 0.8424 recall: 0.8681 specificity: 0.8167 \n",
      "trying epoch loss\n",
      "TP: tensor(4366)\n",
      "TN: tensor(3053)\n",
      "FN: tensor(493)\n",
      "FP: tensor(787)\n",
      "val Loss: 0.0736 Acc: 0.8529 Auc: 0.8468 recall: 0.8985 specificity: 0.7951 \n",
      "Epoch 28/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9610)\n",
      "TN: tensor(6924)\n",
      "FN: tensor(1390)\n",
      "FP: tensor(1516)\n",
      "train Loss: 0.0624 Acc: 0.8505 Auc: 0.8470 recall: 0.8736 specificity: 0.8204 \n",
      "trying epoch loss\n",
      "TP: tensor(4301)\n",
      "TN: tensor(3104)\n",
      "FN: tensor(558)\n",
      "FP: tensor(736)\n",
      "val Loss: 0.0723 Acc: 0.8512 Auc: 0.8467 recall: 0.8852 specificity: 0.8083 \n",
      "Epoch 29/29\n",
      "----------\n",
      "trying epoch loss\n",
      "TP: tensor(9603)\n",
      "TN: tensor(6900)\n",
      "FN: tensor(1397)\n",
      "FP: tensor(1540)\n",
      "train Loss: 0.0632 Acc: 0.8489 Auc: 0.8453 recall: 0.8730 specificity: 0.8175 \n",
      "trying epoch loss\n",
      "TP: tensor(4315)\n",
      "TN: tensor(3092)\n",
      "FN: tensor(544)\n",
      "FP: tensor(748)\n",
      "val Loss: 0.0722 Acc: 0.8515 Auc: 0.8466 recall: 0.8880 specificity: 0.8052 \n",
      "Training complete in 144m 41s\n",
      "Best val Acc: 0.854351\n",
      "returning and looping back\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_ft_vgg =torchvision.models.vgg16(pretrained=True).to(device)\n",
    "num_ftrs = model_ft_vgg.classifier[0].in_features\n",
    "feature_model = list(model_ft_vgg.classifier[:-6].children())\n",
    "feature_model.pop()            \n",
    "feature_model.append(torch.nn.Linear(num_ftrs, NUM_CLASSES))\n",
    "model_ft_vgg.classifier = torch.nn.Sequential(*feature_model).to(device)\n",
    "for param in model_ft_vgg.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = FocalLoss(gamma=0)\n",
    "\n",
    "if use_gpu:\n",
    "    criterion=criterion.cuda()\n",
    "    model_ft_vgg = model_ft_vgg.cuda()\n",
    "\n",
    "optimizer_ft = torch.optim.RMSprop(model_ft_vgg.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft_vgg, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)\n",
    "\n",
    "# Save model\n",
    "model_ft.save_state_dict('models/fine_tuned_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-00127c5ae41a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Run the functions and save the best model in the function model_ft.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m model_ft = train_model(model_ft_vgg, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[1;32m---> 23\u001b[1;33m                        num_epochs=30)\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-13d1804bc9a2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, lr_scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[0mphase_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                     \u001b[0mphase_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                     \u001b[0mphase_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;31m# TP    predict 和 label 同时为1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_ft_vgg =torchvision.models.vgg16(pretrained=True).to(device)\n",
    "num_ftrs = model_ft_vgg.classifier[0].in_features\n",
    "feature_model = list(model_ft_vgg.classifier[:-6].children())\n",
    "feature_model.pop()            \n",
    "feature_model.append(torch.nn.Linear(num_ftrs, NUM_CLASSES))\n",
    "model_ft_vgg.classifier = torch.nn.Sequential(*feature_model).to(device)\n",
    "for param in model_ft_vgg.features.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = FocalLoss(gamma=0)\n",
    "\n",
    "if use_gpu:\n",
    "    criterion=criterion.cuda()\n",
    "    model_ft_vgg = model_ft_vgg.cuda()\n",
    "\n",
    "optimizer_ft = torch.optim.RMSprop(model_ft_vgg.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft_vgg, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)\n",
    "\n",
    "# Save model\n",
    "torch.save(model_ft.state_dict(), 'models/fine_tuned_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0.25, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
