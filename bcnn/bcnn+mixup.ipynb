{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模块\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "import torch\n",
    "import torchvision\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from fine_tuning_config_file import *\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#采用gpu训练模型\n",
    "use_gpu = GPU_MODE\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据转换，为了扩增数据\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "#加载数据集\n",
    "data_dir = DATA_DIR\n",
    "dsets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "         for x in ['train', 'val']}\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=25)\n",
    "                for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据增强\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练网络模型\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                mode='train'\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                mode='val'\n",
    "\n",
    "            train_loss = 0\n",
    "            reg_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            counter=0\n",
    "            # Iterate over data.\n",
    "            for batch_idx, (inputs, targets) in enumerate(dset_loaders[phase]):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, 0.2, use_cuda=True)\n",
    "                inputs, targets_a, targets_b = map(Variable, (inputs,targets_a, targets_b))\n",
    "                outputs = model(inputs)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                if counter==0:\n",
    "                    phase_pred=predicted.cpu().numpy()\n",
    "                    phase_label_a=targets_a.cpu().numpy()\n",
    "                    phase_label_b=targets_b.cpu().numpy()\n",
    "                else:\n",
    "                    phase_pred = np.append(phase_pred, predicted.cpu().numpy())\n",
    "                    phase_label_a = np.append(phase_label_a,targets_a.cpu().numpy())\n",
    "                    phase_label_b = np.append(phase_label_b,targets_b.cpu().numpy())\n",
    "                total += targets.size(0)\n",
    "                correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
    "                counter+=1\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            epoch_loss = train_loss/(batch_idx+1)\n",
    "            epoch_acc = 100.*correct/total\n",
    "            epoch_auc = (lam*roc_auc_score(phase_label_a, phase_pred)+(1-lam)*roc_auc_score(phase_label_b, phase_pred))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Auc: {:.4f} '.format(\n",
    "                phase, epoch_loss,epoch_acc,epoch_auc))\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                \"\"\"if USE_TENSORBOARD:\n",
    "                    foo.add_scalar_value('epoch_loss',epoch_loss,step=epoch)\n",
    "                    foo.add_scalar_value('epoch_acc',epoch_acc,step=epoch)\"\"\"\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    print('new best accuracy = ',best_acc)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改变训练模型的学习率\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用迁移学习搭建自己的网络\n",
    "class BCNN(torch.nn.Module):\n",
    "    \"\"\"B-CNN for CUB200.\n",
    "\n",
    "    The B-CNN model is illustrated as follows.\n",
    "    conv1^2 (64) -> pool1 -> conv2^2 (128) -> pool2 -> conv3^3 (256) -> pool3\n",
    "    -> conv4^3 (512) -> pool4 -> conv5^3 (512) -> bilinear pooling\n",
    "    -> sqrt-normalize -> L2-normalize -> fc (200).\n",
    "    The network accepts a 3*448*448 input, and the pool5 activation has shape\n",
    "    512*28*28 since we down-sample 5 times.\n",
    "    vgg16：conv1^2 (64) -> pool1 -> conv2^2 (128) -> pool2 -> conv3^3 (256) -> pool3\n",
    "    -> conv4^3 (512) -> pool4 -> conv5^3 (512) \n",
    "    Attributes:\n",
    "        features, torch.nn.Module: Convolution and pooling layers.\n",
    "        fc, torch.nn.Module: 200.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        torch.nn.Module.__init__(self)\n",
    "        # Convolution and pooling layers of VGG-16.\n",
    "        self.features = torchvision.models.vgg16(pretrained=True,progress=True).features#获取vgg16的特征\n",
    "        self.features = torch.nn.Sequential(*list(self.features.children())#获取零到最后第二层的特征\n",
    "                                            [:-1])  # Remove pool5.\n",
    "        # Linear classifier.\n",
    "        self.fc = torch.nn.Linear(512**2, 2)#定义一个输入特征为512*2，输出特征为200的线性层\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            X, torch.autograd.Variable of shape N*3*448*448.\n",
    "\n",
    "        Returns:\n",
    "            Score, torch.autograd.Variable of shape N*200.\n",
    "        \"\"\"\n",
    "        N = X.size()[0]#n为x的第一维形状\n",
    "        assert X.size() == (N, 3, 224, 224)#判断x的形状是否为（n，3，448，448）\n",
    "        X = self.features(X)#？？\n",
    "        assert X.size() == (N, 512, 14, 14)#判断x的形状是否为（n，512，28，28）\n",
    "        X = X.view(N, 512, 14**2)#转换x的形状为(n，512，28*28)\n",
    "        X = torch.bmm(X, torch.transpose(X, 1, 2)) / (14**2)  # Bilinear  x矩阵乘x的转置然后除28*28\n",
    "        assert X.size() == (N, 512, 512)#判断x的形状是否为\n",
    "        X = X.view(N, 512**2)\n",
    "        X = torch.sqrt(X + 1e-5)#开根号\n",
    "        X = torch.nn.functional.normalize(X)#进行参数规范化\n",
    "        X = self.fc(X)#加入线性层，输出为（n，200）\n",
    "        assert X.size() == (N, 2)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0.25, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.5209 Acc: 74.3706 Auc: 0.6508 \n",
      "val Loss: 0.4086 Acc: 82.0247 Auc: 0.6914 \n",
      "new best accuracy =  tensor(82.0247)\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.4947 Acc: 76.2022 Auc: 0.6608 \n",
      "val Loss: 0.3711 Acc: 84.4089 Auc: 0.6973 \n",
      "new best accuracy =  tensor(84.4089)\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.4862 Acc: 76.9466 Auc: 0.6675 \n",
      "val Loss: 0.3508 Acc: 86.2379 Auc: 0.7251 \n",
      "new best accuracy =  tensor(86.2379)\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.4763 Acc: 77.4599 Auc: 0.6699 \n",
      "val Loss: 0.3386 Acc: 86.4548 Auc: 0.7226 \n",
      "new best accuracy =  tensor(86.4548)\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.4778 Acc: 77.2199 Auc: 0.6654 \n",
      "val Loss: 0.3279 Acc: 87.8362 Auc: 0.7284 \n",
      "new best accuracy =  tensor(87.8362)\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.4683 Acc: 78.0591 Auc: 0.6717 \n",
      "val Loss: 0.3119 Acc: 88.2741 Auc: 0.7383 \n",
      "new best accuracy =  tensor(88.2741)\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.4661 Acc: 78.1618 Auc: 0.6730 \n",
      "val Loss: 0.3113 Acc: 88.6732 Auc: 0.7163 \n",
      "new best accuracy =  tensor(88.6732)\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.4647 Acc: 78.3834 Auc: 0.6707 \n",
      "val Loss: 0.3043 Acc: 89.0504 Auc: 0.7442 \n",
      "new best accuracy =  tensor(89.0504)\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.4603 Acc: 78.6159 Auc: 0.6740 \n",
      "val Loss: 0.2947 Acc: 89.7516 Auc: 0.7385 \n",
      "new best accuracy =  tensor(89.7516)\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.4611 Acc: 78.5483 Auc: 0.6758 \n",
      "val Loss: 0.3031 Acc: 89.0521 Auc: 0.7337 \n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.4628 Acc: 78.4886 Auc: 0.6755 \n",
      "val Loss: 0.2864 Acc: 90.0185 Auc: 0.7405 \n",
      "new best accuracy =  tensor(90.0185)\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.4525 Acc: 78.9693 Auc: 0.6725 \n",
      "val Loss: 0.2899 Acc: 90.0143 Auc: 0.7420 \n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.4565 Acc: 78.8108 Auc: 0.6760 \n",
      "val Loss: 0.2800 Acc: 90.4892 Auc: 0.7366 \n",
      "new best accuracy =  tensor(90.4892)\n",
      "Epoch 13/29\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model_ft = torch.nn.DataParallel(BCNN()).cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = FocalLoss()\n",
    "\n",
    "if use_gpu:\n",
    "    criterion.cuda()\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "optimizer_ft = torch.optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)\n",
    "\n",
    "# Save model\n",
    "#model_ft.save_state_dict('fine_tuned_best_model.pt')\n",
    "torch.save(model_ft.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.5210 Acc: 74.5567 Auc: 0.6474 \n",
      "val Loss: 0.4041 Acc: 82.3743 Auc: 0.7028 \n",
      "new best accuracy =  tensor(82.3743)\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.4937 Acc: 76.4639 Auc: 0.6582 \n",
      "val Loss: 0.3829 Acc: 84.1254 Auc: 0.7029 \n",
      "new best accuracy =  tensor(84.1254)\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.4851 Acc: 76.8511 Auc: 0.6621 \n",
      "val Loss: 0.3543 Acc: 85.6500 Auc: 0.7262 \n",
      "new best accuracy =  tensor(85.6500)\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.4793 Acc: 77.3808 Auc: 0.6671 \n",
      "val Loss: 0.3544 Acc: 86.1378 Auc: 0.7321 \n",
      "new best accuracy =  tensor(86.1378)\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.4726 Acc: 77.8167 Auc: 0.6676 \n",
      "val Loss: 0.3306 Acc: 87.2180 Auc: 0.7244 \n",
      "new best accuracy =  tensor(87.2180)\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.4703 Acc: 78.0477 Auc: 0.6667 \n",
      "val Loss: 0.3075 Acc: 88.4848 Auc: 0.7215 \n",
      "new best accuracy =  tensor(88.4848)\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.4671 Acc: 78.3398 Auc: 0.6715 \n",
      "val Loss: 0.3065 Acc: 88.8410 Auc: 0.7336 \n",
      "new best accuracy =  tensor(88.8410)\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.4640 Acc: 78.1826 Auc: 0.6659 \n",
      "val Loss: 0.3018 Acc: 89.3314 Auc: 0.7472 \n",
      "new best accuracy =  tensor(89.3314)\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.4625 Acc: 78.3306 Auc: 0.6743 \n",
      "val Loss: 0.3040 Acc: 89.2154 Auc: 0.7407 \n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.4643 Acc: 78.3269 Auc: 0.6696 \n",
      "val Loss: 0.2993 Acc: 89.4571 Auc: 0.7348 \n",
      "new best accuracy =  tensor(89.4571)\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.4608 Acc: 78.6236 Auc: 0.6676 \n",
      "val Loss: 0.2910 Acc: 89.8810 Auc: 0.7506 \n",
      "new best accuracy =  tensor(89.8810)\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.4563 Acc: 79.0135 Auc: 0.6759 \n",
      "val Loss: 0.2867 Acc: 89.9302 Auc: 0.7414 \n",
      "new best accuracy =  tensor(89.9302)\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.4541 Acc: 79.0301 Auc: 0.6738 \n",
      "val Loss: 0.2836 Acc: 90.4832 Auc: 0.7556 \n",
      "new best accuracy =  tensor(90.4832)\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.4549 Acc: 78.9706 Auc: 0.6737 \n",
      "val Loss: 0.2738 Acc: 90.8610 Auc: 0.7438 \n",
      "new best accuracy =  tensor(90.8610)\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.4589 Acc: 78.5141 Auc: 0.6705 \n",
      "val Loss: 0.2675 Acc: 91.4361 Auc: 0.7465 \n",
      "new best accuracy =  tensor(91.4361)\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.4541 Acc: 78.9127 Auc: 0.6731 \n",
      "val Loss: 0.2685 Acc: 90.9860 Auc: 0.7569 \n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.4544 Acc: 78.7810 Auc: 0.6745 \n",
      "val Loss: 0.2688 Acc: 90.9361 Auc: 0.7567 \n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.4525 Acc: 79.0143 Auc: 0.6734 \n",
      "val Loss: 0.2563 Acc: 91.7791 Auc: 0.7610 \n",
      "new best accuracy =  tensor(91.7791)\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.4533 Acc: 78.9576 Auc: 0.6718 \n"
     ]
    }
   ],
   "source": [
    "model_ft = torch.nn.DataParallel(BCNN()).cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = FocalLoss(gamma=0)\n",
    "\n",
    "if use_gpu:\n",
    "    criterion.cuda()\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "optimizer_ft = torch.optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)\n",
    "\n",
    "# Save model\n",
    "#model_ft.save_state_dict('fine_tuned_best_model.pt')\n",
    "torch.save(model_ft.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "LR is set to 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4373 Acc: 74.6363 Auc: 0.6482 \n",
      "val Loss: 0.3439 Acc: 82.3653 Auc: 0.7006 \n",
      "new best accuracy =  tensor(82.3653)\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.4138 Acc: 76.5494 Auc: 0.6616 \n",
      "val Loss: 0.3173 Acc: 84.3809 Auc: 0.6992 \n",
      "new best accuracy =  tensor(84.3809)\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.4080 Acc: 76.8502 Auc: 0.6607 \n",
      "val Loss: 0.2894 Acc: 86.3732 Auc: 0.7201 \n",
      "new best accuracy =  tensor(86.3732)\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.4025 Acc: 77.2298 Auc: 0.6657 \n",
      "val Loss: 0.2832 Acc: 87.3000 Auc: 0.7276 \n",
      "new best accuracy =  tensor(87.3000)\n",
      "Epoch 4/29\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-da2f597bb6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Run the functions and save the best model in the function model_ft.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[1;32m---> 15\u001b[1;33m                        num_epochs=30)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2a4417eefa0f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, lr_scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixup_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = torch.nn.DataParallel(BCNN()).cuda()\n",
    "\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss()\n",
    "\n",
    "if use_gpu:\n",
    "    criterion.cuda()\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "optimizer_ft = torch.optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)\n",
    "\n",
    "# Save model\n",
    "#model_ft.save_state_dict('fine_tuned_best_model.pt')\n",
    "torch.save(model_ft.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
